I"h4<h1 id="pegasuspre-training-with-extracted-gap-sentences-for-abstractive-summarization">PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization</h1>

<p><a href="https://arxiv.org/pdf/1912.08777.pdf">논문 URL</a></p>

<h2 id="abstract">Abstract</h2>
<p>추상적 요약(Abstractive Summarization)에 맞는 사전학습 모델인 PEGASUS를 제안한다. PEGASUS 모델은 트랜스포머(Transformer) 기반의 인코더-디코더 구조이고, 사전 학습을 위한 새로운 기법인 GSG도 함께 제안한다.</p>

<p><img src="https://user-images.githubusercontent.com/63278762/184355012-a2e66abe-7c5d-4444-9956-81320a21aa9d.png" alt="image" /></p>

<p>기본 구조는 표준 트랜스포머 인코더-디코더 구조이다.  위의 그림에서는 문장 1개는 [MASK1]로 마스킹 되어 Target 텍스트 생성에 사용하고, 다른 두 문장에서 랜덤 한 토큰을 [MASK2]로 마스킹 한다.</p>

<h2 id="introduction">Introduction</h2>

<blockquote>
  <p>추상적 요약이란?  <br />
기존 문장에서 요약문을 뽑아내는 것이 아닌, 요약 내용을 담은 새로운 문장을 생성하는 것</p>
</blockquote>

<p>이 논문에서는 아래와 같은 내용을 소개할 예정이다.</p>

<p>​</p>
<ol>
  <li>
    <p>추상적 요약을 위한 새로운 self-supervised 사전학습 모델과 Gap-Sentences Generation(GSG)를 제안하고, 문장을 선택하는 전략을 연구한다.</p>
  </li>
  <li>
    <p>제안된 사전학습 모델이 넓은 범위의 Downstream에서도 좋은 성능을 내는지 평가한다.</p>
  </li>
  <li>
    <p>fine-tuning 한 PEGASUS 모델이 label이 없는 큰 도메인에서 어떻게 좋은 성능을 달성하는지 소개한다.</p>
  </li>
  <li>
    <p>모델을 검증하고, 인간 수준의 요약 성능을 보여주기 위해 사람 평가 연구를 진행한다. (XSum, CNN/DailyMail, Reddit TIFU 데이터 셋에서)</p>
  </li>
</ol>

<h2 id="pre-training-objectives">Pre-training Objectives</h2>
<p>(비교를 위해 BERT의 masked-language model 방식을 함께 살펴보았다.)</p>

<h3 id="gap-sentences-generation-gsg">Gap Sentences Generation (GSG)</h3>
<ul>
  <li>추상적 요약을 위해 제안된 새로운 방식의 self-supervised pre-training objective</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/63278762/184356377-e22fe230-1de7-4824-8daf-9f34efb6b569.gif" alt="image1" /></p>

<ol>
  <li>GSR(Gap Sentences Ratio)에 따라 문장을 선택하고 마스킹</li>
  <li>선택된 문장을 연결하여 pseudo-요약문으로 만듦.</li>
  <li>선택된 문장 위치에 [MASK1]토큰을 넣은 후 모델에 넣는다.</li>
  <li>[MASK1] 토큰을 예측하는 방식으로 학습한다.</li>
</ol>

<blockquote>
  <p>GSR이란?  <br />
문서 내 전체 문장 개수 대비 선택된 문장의 개수 비율</p>
</blockquote>

<h4 id="문장을-선택하는-전략">문장을 선택하는 전략</h4>
<p>논문에서는 문장을 선택하는 전략을 세 가지 소개하고 있다.</p>

<ol>
  <li>Random : 랜덤하게 m 개의 문장을 선택한다.</li>
  <li>Lead : 처음 m 개의 문장을 선택한다.</li>
  <li>Principal : 중요도에 따른 상위 m 개의 문장을 선택한다. 이때 중요도는 문장과 나머지 문서 사이의 ROUGE1-F1을 계산해서 사용한다.
    <ul>
      <li>옵션1: Ind or Seq -&gt; 문장을 순서와 관계없이 선택 또는 연속적으로 선택</li>
      <li>옵션2: Uniq or Orig -&gt; n-gram을 집합으로 취급 또는 중복을 허용</li>
    </ul>
  </li>
</ol>

<p><img src="https://user-images.githubusercontent.com/63278762/184357602-cfb2a3fe-500d-47df-bc83-e73459af8920.png" alt="image" /></p>

<h3 id="masked-language-model-mlm">Masked Language Model (MLM)</h3>
<p>입력 텍스트에서 15% 토큰을 랜덤하게 선택한다. 이 토큰은 다음 3가지 과정 중 한 가지를 거치면서 마스킹 된다.</p>
<blockquote>
  <ol>
    <li>80% 확률로 [MASK2] 토큰으로 대체된다.</li>
    <li>10% 확률로 임의의 토큰으로 대체된다.</li>
    <li>10% 확률로 그대로 유지된다.</li>
  </ol>
</blockquote>

<p><em>이것에 대한 자세한 내용은 BERT 논문에 나와있다.</em></p>

<p><img src="https://user-images.githubusercontent.com/63278762/184357959-83a414cf-f591-4aa4-9601-e171160a21c5.png" alt="image" /></p>

<p>위의 구조에서는 GSG와 MLM을 동시에 적용하였지만, MLM이 Downstream Task의 성능을 개선하지 못했고, 이 논문의 최종 모델인 PEGASUS-Large에 적용하지 않았다.</p>

<h2 id="pre-training-corpus">Pre-training Corpus</h2>
<ul>
  <li>C4</li>
  <li>HugeNews</li>
</ul>

<h2 id="downstream-tasksdatasets">Downstream Tasks/Datasets</h2>
<ul>
  <li>XSum, CNN/DailyMail, NEWSROOM, Multi-News, Gigaword, WikiHow, Reddit TIFU, BIGPATENT, arXiv, PubMed, AESLC, BillSum</li>
  <li>총 12개 사용했다.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">Encoder, Decoder Layer 개수</th>
      <th style="text-align: right">Hidden Size</th>
      <th style="text-align: right">Feed-Forward Layer 개수</th>
      <th style="text-align: right">Self-Attention Head 개수</th>
      <th style="text-align: right">Batch Size</th>
      <th style="text-align: right">파라미터 수</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">PEGASUS-Base</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">768</td>
      <td style="text-align: right">3072</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">256</td>
      <td style="text-align: right">223M</td>
    </tr>
    <tr>
      <td style="text-align: right">PEGASUS-Large</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">1024</td>
      <td style="text-align: right">4096</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">8192</td>
      <td style="text-align: right">568M</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Optimizer: Adafactor</li>
  <li>Dropout rate: 0.1
    <blockquote>
      <p>Adafactor이란?  <br />
메모리 사용량을 줄일 수 있는 Adam기반의 Optimizer</p>
    </blockquote>
  </li>
</ul>

<p>효율적인 실험을 위해 우선 4개의 데이터 셋을 이용해 실험을 진행했다.</p>
<ul>
  <li>XSum</li>
  <li>CNN/DailyMail</li>
  <li>WikiHow</li>
  <li>Reddit TIFU</li>
</ul>

<h3 id="pegasus-base">PEGASUS-BASE</h3>
<p>시간과 자원을 절약하기 위해 사이즈를 줄인 PEGASUS-Base를 이용해 평가했다. 
평가항목으로는 다음 세가지이다.</p>

<ul>
  <li>Pre-training Corpus</li>
  <li>Pre-training Objectives</li>
  <li>단어장 사이즈</li>
</ul>

<h4 id="pre-training-corpus-1">Pre-training Corpus</h4>
<p><img src="https://user-images.githubusercontent.com/63278762/184359716-ca350c72-d7a7-40f8-8ef9-7eff0f6c7c88.png" alt="image" /></p>
<ul>
  <li>뉴스가 아닌 데이터셋 : C4가 효율적</li>
  <li>뉴스 DownStream 데이터셋 : HugeNews가 효율적
    <h4 id="pre-training-objectives-1">Pre-training Objectives</h4>
    <p><img src="https://user-images.githubusercontent.com/63278762/184359776-0cfc6628-78f5-42ba-96b6-8509d9918a1a.png" alt="image" /></p>
  </li>
</ul>

<p><strong>Effect of pre-training objectives</strong></p>
<ul>
  <li>30%의 GSR에서 실험</li>
  <li>GSG : Ind-Orig &gt; Seq-Uniq 순으로 성능이 좋음</li>
  <li>MLM : 단독으로 사용했을 경우 성능이 더 낮았고, GSG와 혼합한 방식도 GSG의 Random 방식과 비슷한 성능을 보임</li>
</ul>

<p><strong>Effect of gap sentences ratio with GSG (Ind-Orig)</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Ind-Orig</code> 방식에서 실험</li>
  <li>공평한 실험을 위해 데이터셋을 최대 400단어를 갖도록 자름</li>
  <li>50%이하일 때 대체적으로 좋은 성능을 보임</li>
  <li>CNN/DailyMail : 15%일때 성능이 좋음</li>
  <li>XSum,Reddit TIFU : 30%일때 성능이 좋음</li>
  <li>WikiHow : 45%일때 성능이 좋음</li>
</ul>

<h4 id="단어장-사이즈">단어장 사이즈</h4>
<p><img src="https://user-images.githubusercontent.com/63278762/184359847-c986ac0a-08df-4aed-9e29-429dc49d3752.png" alt="image" /></p>
<ul>
  <li>두가지 토크나이저를 사용 (Byte-pair-encoding = BPE, SentencePiece Unigram = Unigram)</li>
  <li>XSum, CNN/DailyMail : Unigram 96k일때 성능이 좋음</li>
  <li>WikiHow : Unigram 128k일때 성능이 좋음</li>
  <li>Reddit TIFU : Unigram 96k일때 성능이 좋음</li>
</ul>

<h3 id="pegasus-large">PEGASUS-Large</h3>
<p>PEGASUS-Base 실험 결과 적용된 것은 다음과 같다.</p>
<ul>
  <li>Pre-training 방식 : GSG(Ind-Orig)</li>
  <li>GSR : 45% (30%와 비슷한 성능을 내기 위해 선택)</li>
  <li>단어장 사이즈 : Unigram 96k</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/63278762/184360583-cfd06a1c-4a0a-44c3-8a92-d068b0fb8d8b.png" alt="image" /></p>

<ul>
  <li>PEGASUS-Base도 대부분의 Downstream에서 현재 SOTA에 비해 좋은 성능을 내고 있지만, PEGASUS-Large의 경우 모든 Downstream에서 SOTA보다 좋은 성능을 내고 있음</li>
  <li>대부분 Downstream은 HugeNews로 사전학습한 모델이 더 높은 성능을 보임</li>
  <li>단, WikiHow의 경우 C4로 사전학습한 모델이 더 높은 성능을 보임</li>
</ul>

<h2 id="zero-and-low-resource-summarization">Zero and Low-Resource Summarization</h2>
<p>현실에서는 대용량의 요약 지도학습 데이터를 모으기가 쉽지 않다. 때문에 이 논문에서는 사전학습을 시킨 모델이 적은 데이터에서도 좋은 성능을 내는지 실험했다.</p>

<p>PEGASUS-Large모델을 fine-tuning 하기 위한 설정은 다음과 같다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">step</th>
      <th style="text-align: right">배치 사이즈</th>
      <th style="text-align: right">learning rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">2000</td>
      <td style="text-align: right">256</td>
      <td style="text-align: right">0.0005</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Trainsformer-base model은 Full supervised dataset으로 fine-tuning이 됨.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/63278762/184361136-869608e6-0347-431e-b1fd-e92a2515ede0.png" alt="image" /></p>

<ul>
  <li>100개의 데이터만으로 20k~200k로 학습한 TransformerBase과 유사한 성능을 낸다.</li>
  <li>1000개의 데이터로 학습했을 때 12개 데이터 셋에서 6개가 기존 SOTA보다 높은 성능을 보인다.</li>
</ul>

<p><strong>위의 결과로 적은 데이터로도 높은 성능을 내는 것을 볼 수 있다.</strong></p>

<h2 id="human-evaluation">Human Evaluation</h2>
<p>이 논문에서는 요약 모델의 결과와 사람이 직접 요약한 결과와 비교하는 실험을 진행했다. 이때 사용한 데이터 셋은 XSum, CNN/DailyMail, Reddit TIFU이다. 
평가 방법은 Amazon Mechanical Turk site를 통해 1점 (Poor) ~ 5점 (Great)까지 점수를 매기도록 진행하였다.</p>

<p><img src="https://user-images.githubusercontent.com/63278762/184361477-5f5ba162-69a8-42ab-b826-54ba23035e83.png" alt="image" /></p>

<h3 id="실험-1">실험 1</h3>
<ul>
  <li>실험 방법: Pegasus-large (HugeNews), Pegasus-large (C4), Transformer-base model이 생성한 요약문과 reference summaries를 비교했다.</li>
  <li>실험 결과: Human evaluation에서, Pegasus-large (HugeNews)와 Pegasus-large (C4) 모두 Human-written summary (3.0 XSum, 3.1 CNNDM, 3.2 Reddit TIFU)와 비슷하거나 높은 score를 기록했다.</li>
</ul>

<h3 id="실험-2">실험 2</h3>
<ul>
  <li>실험 방법: Pegasus-large (HugeNews)는 10, 100, 1000 examples로 fine-tuning한 후, 각 model이 생성한 요약문과 reference summaries를 비교했다.</li>
  <li>실험 결과:
    <ul>
      <li>XSum, CNNDM dataset은 10 ~ 1000 examples로만 학습한 Pegasus-large model이여도 Human-written과 유사하거나 높은 score를 기록했다.</li>
      <li>Reddit TIFU에서는 full supervision dataset으로 학습한 model만 유사한 score를 기록하였는데, dataset의 특성이 문서 스타일이 많이 달랐기 때문에 모델의 요약본의 점수가 많이 떨어졌다.</li>
    </ul>
  </li>
</ul>

<h2 id="pegasus-large의-성능-향상">PEGASUS-Large의 성능 향상</h2>
<p>PEGASUS-Large의 성능을 높이기 위해 C4와 HugeNews 데이터를 함께 사전학습시켜서 실험했다.</p>

<p><img src="https://user-images.githubusercontent.com/63278762/184362071-a7d9bb7e-64f7-4f9e-8af9-0035fa3a6783.png" alt="image" /></p>

<p>그 결과 모든 Downstream Task에서 가장 좋은 성능을 냈다.</p>

<h2 id="결론">결론</h2>
<ol>
  <li>추상적 요약을 위한 사전학습 모델인 PEGASUS를 소개했고, 이 모델은 새로운 사전학습 기법인 GSG를 이용해 학습된다.</li>
  <li>사전학습된 모델은 적은 Downstream 데이터여도 좋은 성능을 낸다.</li>
  <li>Human Evaluation를 통해 모델의 요약문이 사람 수준을 달성함을 보인다.</li>
</ol>
:ET