I"<h1 id="pegasuspre-training-with-extracted-gap-sentences-for-abstractive-summarization">PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization</h1>

<p><a href="https://arxiv.org/pdf/1912.08777.pdf">논문 URL</a></p>

<h2 id="abstract">Abstract</h2>
<p>추상적 요약(Abstractive Summarization)에 맞는 사전학습 모델인 PEGASUS를 제안한다. PEGASUS 모델은 트랜스포머(Transformer) 기반의 인코더-디코더 구조이고, 사전 학습을 위한 새로운 기법인 GSG도 함께 제안한다.</p>

<p><img src="https://user-images.githubusercontent.com/63278762/184355012-a2e66abe-7c5d-4444-9956-81320a21aa9d.png" alt="image" /></p>

<p>기본 구조는 표준 트랜스포머 인코더-디코더 구조이다.  위의 그림에서는 문장 1개는 [MASK1]로 마스킹 되어 Target 텍스트 생성에 사용하고, 다른 두 문장에서 랜덤 한 토큰을 [MASK2]로 마스킹 한다.</p>

<h2 id="introduction">Introduction</h2>

<blockquote>
  <p>추상적 요약이란?  <br />
기존 문장에서 요약문을 뽑아내는 것이 아닌, 요약 내용을 담은 새로운 문장을 생성하는 것</p>
</blockquote>

<p>이 논문에서는 아래와 같은 내용을 소개할 예정이다.</p>

<p>​</p>
<ol>
  <li>
    <p>추상적 요약을 위한 새로운 self-supervised 사전학습 모델과 Gap-Sentences Generation(GSG)를 제안하고, 문장을 선택하는 전략을 연구한다.</p>
  </li>
  <li>
    <p>제안된 사전학습 모델이 넓은 범위의 Downstream에서도 좋은 성능을 내는지 평가한다.</p>
  </li>
  <li>
    <p>fine-tuning 한 PEGASUS 모델이 label이 없는 큰 도메인에서 어떻게 좋은 성능을 달성하는지 소개한다.</p>
  </li>
  <li>
    <p>모델을 검증하고, 인간 수준의 요약 성능을 보여주기 위해 사람 평가 연구를 진행한다. (XSum, CNN/DailyMail, Reddit TIFU 데이터 셋에서)</p>
  </li>
</ol>

<h2 id="pre-training-objectives">Pre-training Objectives</h2>
<p>(비교를 위해 BERT의 masked-language model 방식을 함께 살펴보았다.)</p>

<h3 id="gap-sentences-generation-gsg">Gap Sentences Generation (GSG)</h3>
<ul>
  <li>추상적 요약을 위해 제안된 새로운 방식의 self-supervised pre-training objective</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/63278762/184356377-e22fe230-1de7-4824-8daf-9f34efb6b569.gif" alt="image1" /></p>

<ol>
  <li>GSR(Gap Sentences Ratio)에 따라 문장을 선택하고 마스킹</li>
  <li>선택된 문장을 연결하여 pseudo-요약문으로 만듦.</li>
  <li>선택된 문장 위치에 [MASK1]토큰을 넣은 후 모델에 넣는다.</li>
  <li>[MASK1] 토큰을 예측하는 방식으로 학습한다.</li>
</ol>

<blockquote>
  <p>GSR이란?  <br />
문서 내 전체 문장 개수 대비 선택된 문장의 개수 비율</p>
</blockquote>

<h4 id="문장을-선택하는-전략">문장을 선택하는 전략</h4>
<p>논문에서는 문장을 선택하는 전략을 세 가지 소개하고 있다.</p>

<ol>
  <li>Random : 랜덤하게 m 개의 문장을 선택한다.</li>
  <li>Lead : 처음 m 개의 문장을 선택한다.</li>
  <li>Principal : 중요도에 따른 상위 m 개의 문장을 선택한다. 이때 중요도는 문장과 나머지 문서 사이의 ROUGE1-F1을 계산해서 사용한다.
    <ul>
      <li>옵션1: Ind or Seq -&gt; 문장을 순서와 관계없이 선택 또는 연속적으로 선택</li>
      <li>옵션2: Uniq or Orig -&gt; n-gram을 집합으로 취급 또는 중복을 허용</li>
    </ul>
  </li>
</ol>

<p><img src="https://user-images.githubusercontent.com/63278762/184357602-cfb2a3fe-500d-47df-bc83-e73459af8920.png" alt="image" /></p>

<h3 id="masked-language-model-mlm">Masked Language Model (MLM)</h3>
<p>입력 텍스트에서 15% 토큰을 랜덤하게 선택한다. 이 토큰은 다음 3가지 과정 중 한 가지를 거치면서 마스킹 된다.</p>
<blockquote>
  <ol>
    <li>80% 확률로 [MASK2] 토큰으로 대체된다.</li>
    <li>10% 확률로 임의의 토큰으로 대체된다.</li>
    <li>10% 확률로 그대로 유지된다.</li>
  </ol>
</blockquote>

<p><em>이것에 대한 자세한 내용은 BERT 논문에 나와있다.</em></p>

<p><img src="https://user-images.githubusercontent.com/63278762/184357959-83a414cf-f591-4aa4-9601-e171160a21c5.png" alt="image" /></p>

<p>위의 구조에서는 GSG와 MLM을 동시에 적용하였지만, MLM이 Downstream Task의 성능을 개선하지 못했고, 이 논문의 최종 모델인 PEGASUS-Large에 적용하지 않았다.</p>

<h2 id="pre-training-corpus">Pre-training Corpus</h2>
<ul>
  <li>C4</li>
  <li>HugeNews</li>
</ul>

<h2 id="downstream-tasksdatasets">Downstream Tasks/Datasets</h2>
<ul>
  <li>XSum, CNN/DailyMail, NEWSROOM, Multi-News, Gigaword, WikiHow, Reddit TIFU, BIGPATENT, arXiv, PubMed, AESLC, BillSum</li>
  <li>총 12개 사용했다.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: right"> </th>
      <th style="text-align: right">Encoder, Decoder Layer 개수</th>
      <th style="text-align: right">Hidden Size</th>
      <th style="text-align: right">Feed-Forward Layer 개수</th>
      <th style="text-align: right">Self-Attention Head 개수</th>
      <th style="text-align: right">Batch Size</th>
      <th style="text-align: right">파라미터 수</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">PEGASUS-Base</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">768</td>
      <td style="text-align: right">3072</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">256</td>
      <td style="text-align: right">223M</td>
    </tr>
    <tr>
      <td style="text-align: right">PEGASUS-Large</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">1024</td>
      <td style="text-align: right">4096</td>
      <td style="text-align: right">16</td>
      <td style="text-align: right">8192</td>
      <td style="text-align: right">568M</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Optimizer: Adafactor</li>
  <li>Dropout rate: 0.1
    <blockquote>
      <p>Adafactor이란?  <br />
메모리 사용량을 줄일 수 있는 Adam기반의 Optimizer</p>
    </blockquote>
  </li>
</ul>

<p>효율적인 실험을 위해 우선 4개의 데이터 셋을 이용해 실험을 진행했다.</p>
<ul>
  <li>XSum</li>
  <li>CNN/DailyMail</li>
  <li>WikiHow</li>
  <li>Reddit TIFU</li>
</ul>

<h3 id="pegasus-base">PEGASUS-BASE</h3>
<p>시간과 자원을 절약하기 위해 사이즈를 줄인 PEGASUS-Base를 이용해 평가했다. 
평가항목으로는 다음 세가지이다.</p>

<ul>
  <li>Pre-training Corpus</li>
  <li>Pre-training Objectives</li>
  <li>단어장 사이즈</li>
</ul>

:ET